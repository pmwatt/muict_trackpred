# -*- coding: utf-8 -*-
"""personal ml_tracks_pao.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19yFMefXSPf63NetKZUbM2TeBs8lFKvQH

# version 2

## dummy data preparation
Don't forget to change the path to your file

why dummy? aka to fix
- removed ranking questions
- ooga booga used almost 50 features
- god look at the column names
- lazy one-hot categorical encoding, my lazy code encoded numerical values too so 1-4 was changed into 0-3
- I have no idea whether the models considered these as continuous numbers (1 less than 2, etc.) or just discrete category values (category 1, category 2, etc.) because we have a mix of both
"""

import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from pprint import pprint
import pickle

# Path to your track prediction csv file
path = 'dataset_v4c.csv'

# Load the dataset
data = pd.read_csv(path, encoding='latin-1')

# Remove special characters and newlines from column names
data.columns = data.columns.str.replace(r'[^\w\s]+', '', regex=True)
data.columns = data.columns.str.replace(r'\n', ' ', regex=True)
data.columns = data.columns.str.strip()

# Encode the target variable - both categorical and numerical?! (dummy)
# reference: https://stackoverflow.com/questions/42196589/any-way-to-get-mappings-of-a-label-encoder-in-python-pandas
for column in data.columns[1:]:
  le = preprocessing.LabelEncoder()
  data[column] = le.fit_transform(data[column])
  le.fit(data[column])
  le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))

# Split the dataset into features and target
X = data.drop('track', axis=1)
y = data['track']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)
print(X_test)
print(type(X_test))

"""## dummy training and evaluation - random forest"""

params = { #random forest's hyperparameters
    'n_estimators': 5000,    # Number of trees in the forest
    'max_depth': 200,      # Maximum depth of the tree
    'min_samples_split': 3, # Minimum number of samples required to split an internal node
    'min_samples_leaf': 2,  # Minimum number of samples required to be at a leaf node
    'random_state': 42      # Random state for reproducibility
}
# Train a Random Forest Classifier
clf = RandomForestClassifier(**params)
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# print each classes' names
print(np.array([clf.classes_]))

y_preds_probs = clf.predict_proba(X_test)
print(y_preds_probs)
print(y_pred)

pickle.dump(clf, open('model.pkl', 'wb'))